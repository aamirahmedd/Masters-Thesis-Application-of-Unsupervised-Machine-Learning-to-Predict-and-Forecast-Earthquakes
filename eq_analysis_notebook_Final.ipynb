{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to true first time the worksheet is run.\n",
    "\n",
    "# need to install Matplotlib, scipy, scikit-learn, tqdm, sklearnex\n",
    "\n",
    "INSTALL = False\n",
    "if INSTALL:\n",
    "    !pip install matplotlib\n",
    "    !pip install scipy\n",
    "    !pip install scikit-learn\n",
    "    !pip install tqdm\n",
    "    !pip install pip install scikit-learn-intelex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdadbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "from dataclasses import dataclass  # for the lowest level class type\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm   # progress bar tool\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()   \n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f27f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLING_RATE = 100\n",
    "BLOCK_SIZE = 500\n",
    "TEST_DATA_HOURS = 48\n",
    "BUTTER_ORDER = 4\n",
    "\n",
    "# dataclass to represent a station\n",
    "@dataclass\n",
    "class EQStation:\n",
    "    name: str\n",
    "    dist_m: float\n",
    "    ele: float\n",
    "    lat: float\n",
    "    lon: float\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Station: {self.name}\\n\" \\\n",
    "               f\"Distance: {self.dist_m} m\\n\" \\\n",
    "               f\"Elevation: {self.ele} m\\n\" \\\n",
    "               f\"Latitude: {self.lat} deg\\n\" \\\n",
    "               f\"Longitude: {self.lon} deg\"\n",
    "\n",
    "\n",
    "# an EQ dataset that represents one event and multiple stations\n",
    "class EQDataSet:\n",
    "    def __init__(self, filename: str = \"\", exclude_file: str = \"\", for_conversion: bool = False):\n",
    "        self.filename = filename\n",
    "        self.stations_data: list[EQStation] = []\n",
    "        self.stations: list[str] = []\n",
    "        self.magnitude: float = 0\n",
    "        self.time: str = \"\"\n",
    "        self.only_include: int = 999\n",
    "\n",
    "        if not for_conversion:\n",
    "            with h5py.File(self.filename, 'r') as file:\n",
    "                self.magnitude = float(file.attrs['mag'])\n",
    "                self.time = file.attrs['time']\n",
    "                for item in file.items():\n",
    "                    data_dict = dict(item[1].attrs)\n",
    "                    for key, value in data_dict.items():\n",
    "                        data_dict[key] = float(value)\n",
    "                    self.stations.append(item[0])\n",
    "                    self.stations_data.append(EQStation(name=item[0], **data_dict))\n",
    "            # create a property self.time_scale that uses self.time\n",
    "            # and the sampling rate of 100Hz to create a time scale\n",
    "            # for the data\n",
    "            # NOTE THIS HAS BEEN SUPERSEDED BY THE USE OF TIME SCALE\n",
    "            # TAKEN FROM CALIBRATION EVENTS BEING FOUND\n",
    "            end_time = pd.to_datetime(self.time)\n",
    "            # uses Z because it doesn't matter which axis\n",
    "            # they're all the same.\n",
    "            self.time_scale = pd.date_range(end=end_time, periods=len(self.get_data(self.stations[0], 'Z')),\n",
    "                                            freq='10ms')\n",
    "\n",
    "    def data_csv(self):\n",
    "        # dataframe with the data from all stations\n",
    "        # and the time scale\n",
    "        df = pd.DataFrame(columns=['Station', 'Location', 'Notes', 'Latitude', 'Longitude'])\n",
    "        with h5py.File(self.filename, 'r') as file:\n",
    "            for station in self.stations_data:\n",
    "                df.loc[len(df)] = [station.name,'','',station.lat,station.lon]\n",
    "        # write df as a csv to file\n",
    "        df.to_csv(\"station_data.csv\")\n",
    "        return df\n",
    "\n",
    "    def __str__(self):\n",
    "        with h5py.File(self.filename, 'r') as file:\n",
    "            ret_str = \"-\" * 30 + \"\\n\"\n",
    "            ret_str += \"Filename: \" + self.filename + \"\\n\"\n",
    "            ret_str += \"-\" * 30 + \"\\n\"\n",
    "            for station in self.stations_data:\n",
    "                ret_str += f\"{station}\\n\"\n",
    "                ret_str += f\"Time: {self.time}\\n\"\n",
    "                ret_str += f\"Magnitude: {self.magnitude}\\n\"\n",
    "                ret_str += f\"Num axes: {file[station.name].shape[0]}\\n\"\n",
    "                ret_str += f\"Num data-points / axis: {file[station.name].shape[1]}\\n\"\n",
    "                ret_str += f\"Num days: {file[station.name].shape[1]/(SAMPLING_RATE*60*60*24)}\\n\"\n",
    "                ret_str += \"#\"*40 + \"\\n\"\n",
    "        return ret_str\n",
    "\n",
    "    # get the data for a specific station and axis\n",
    "    def get_data(self, station: str, axis_label: str, start_hour: float = 0,\n",
    "                 num_hours: float = 0,\n",
    "                 exclude_file: str = \"\"):\n",
    "        axis_dict = {'N': 0, 'E': 1, 'Z': 2}\n",
    "        axis = axis_dict[axis_label]\n",
    "        start = int(start_hour*3600*SAMPLING_RATE)\n",
    "        end = int((start_hour+num_hours)*3600*SAMPLING_RATE)\n",
    "        with h5py.File(self.filename, 'r') as file:\n",
    "            for s in self.stations_data:\n",
    "                if s.name == station:\n",
    "                    if num_hours:\n",
    "                        raw_data = file[s.name][axis, start:end]\n",
    "                    else:\n",
    "                        raw_data = file[s.name][axis, start:]\n",
    "                    if exclude_file:\n",
    "                        raw_data = self.exclude_marked_up(raw_data, exclude_file=exclude_file)\n",
    "                    return raw_data\n",
    "        return None\n",
    "\n",
    "    # THIS HAS BEEN SUPERSEDED BY THE AUTO-DETECTION OF CALIBRATION EVENTS\n",
    "    @staticmethod\n",
    "    def exclude_marked_up(raw_data, exclude_file: str):\n",
    "        print(\"Excluding marked-up data events\")\n",
    "        # read csv into pandas dataframe\n",
    "        df = pd.read_csv(exclude_file)  # , header=None)\n",
    "        # read in event mark ups\n",
    "        small_markups = df.iloc[4:, 6:8]\n",
    "        # exclude rows of nans\n",
    "        small_markups = small_markups.dropna()\n",
    "        # exclude from data all the rows in small_markups\n",
    "        padding = 500000\n",
    "        # about 1.4 hours either side\n",
    "        # of marked up event\n",
    "        # initialise cleaned data with first chunk\n",
    "        # up to first event marked up\n",
    "        start, prev_end = small_markups.values[0]\n",
    "        prev_end = int(prev_end) + padding\n",
    "        start = max(int(start) - padding, 0)\n",
    "        new_data = raw_data[:int(start)]\n",
    "        for start, end in small_markups.values[1:]:\n",
    "            start = max(int(start) - padding, 0)\n",
    "            end = int(end) + padding\n",
    "            # delete the data in the range\n",
    "            try:\n",
    "                new_data = np.append(new_data, raw_data[int(prev_end):int(start)])\n",
    "                prev_end = end\n",
    "            except IndexError:\n",
    "                pass\n",
    "        return new_data\n",
    "\n",
    "    # plot the data for a specific station and axis for a given time period\n",
    "    def plot_datum(self, station, axis_label, start_hour, num_hours):\n",
    "        d = self.get_data(station, axis_label, start_hour, num_hours)\n",
    "        plt.plot(d)\n",
    "        plt.title(station + ' ' + axis_label)\n",
    "        plt.xlabel('Sample Index (t*100)')\n",
    "        plt.ylabel('Sample Value')\n",
    "        plt.show()\n",
    "\n",
    "    # plot the data for a specific station and all axes for a given time period\n",
    "    def plot_data(self, station, start_hour, num_hours,\n",
    "                  separate_axes=False):\n",
    "        axes = ['N', 'E', 'Z']\n",
    "        d = []\n",
    "        for axis in axes:\n",
    "            d.append(self.get_data(station, axis, start_hour, num_hours))\n",
    "        x_data = 60*60*np.linspace(start_hour, start_hour+num_hours, len(d[0]))\n",
    "        if not separate_axes:\n",
    "            plt.plot(x_data,d[0], label='N')\n",
    "            plt.plot(x_data,d[1], label='E')\n",
    "            plt.plot(x_data,d[2], label='Z')\n",
    "            plt.legend()\n",
    "            # set the axis labels and title\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Speed (nm/s)')\n",
    "            plt.title(f'Sensor Data for {station}')\n",
    "        else:\n",
    "            fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "            for i, ax in enumerate(axes):\n",
    "                axs[i].plot(x_data, d[i])\n",
    "                axs[i].set_xlabel('Time (s)')\n",
    "                axs[i].set_ylabel('Speed (nm/s)')\n",
    "                axs[i].set_title(f'Sensor Data for {axes[i]}({station})')\n",
    "            # Adjusting the spacing between subplots\n",
    "            plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # returns a numpy array of the concatenated data for all stations\n",
    "    def concatenate_station_data(self, sensor, start_hour, num_hours,\n",
    "                                 exclude_file: str = \"\",\n",
    "                                 only_include: int=999):\n",
    "        print(\"Concatenating data...\")\n",
    "        self.only_include = only_include\n",
    "        data = []\n",
    "        include_count = 0\n",
    "        for station in self.stations:\n",
    "            data.append(self.get_data(station, sensor, start_hour, num_hours, exclude_file))\n",
    "            include_count += 1\n",
    "            if include_count >= only_include:\n",
    "                break\n",
    "        return np.concatenate(data)\n",
    "\n",
    "\n",
    "# this class is used to represent a dataset that is a combination of\n",
    "# multiple stations' data but allows it to be pre-limited\n",
    "# in time to avoid huge datasets.\n",
    "# NOTE: I now use a sort of hack with this and a later class\n",
    "class EQDataSetSingleSensorCombined(EQDataSet):\n",
    "    def __init__(self, filename, axis='Z', start_hour=0, num_hours=0,\n",
    "                 exclude_file: str=\"\", only_include: int = 999, for_conversion=False):\n",
    "        super().__init__(filename,\n",
    "                         for_conversion=for_conversion)\n",
    "        self.axis: str = axis\n",
    "        self.start_hour: float = start_hour\n",
    "        self.num_hours: float = num_hours\n",
    "        # conversion is for turning json generated data into a EQDataSetSingleSensorCombined\n",
    "        # it is done by a function in the JSON dataset class\n",
    "        if not for_conversion:\n",
    "            self.combined_data = self.concatenate_station_data(axis,\n",
    "                                                               self.start_hour,\n",
    "                                                               self.num_hours,\n",
    "                                                               exclude_file=exclude_file,\n",
    "                                                               only_include=only_include)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        ret_str = \"Filename: \" + self.filename + \"\\n\"\n",
    "        ret_str += \"Combination of \" + str(len(self.stations)) + \" stations' data\\n\"\n",
    "        ret_str += \"Station names: \" + str(self.stations) + \"\\n\"\n",
    "        ret_str += f\"Axis: {self.axis}\\n\"\n",
    "        ret_str += f\"Combined data shape: {self.combined_data.shape}\\n\"\n",
    "        ret_str += f\"Start hour: {self.start_hour}\\n\"\n",
    "        ret_str += f\"Num hours: {self.num_hours}\\n\"\n",
    "        return ret_str\n",
    "\n",
    "\n",
    "# THIS CLASS IS USED TO BUILD EXPERIMENT DATA FROM MULTIPLE\n",
    "# EVENTS AND STATIONS USING A SPECIFIC JSON FORMAT.\n",
    "# an EQ dataset JSON can have multiple event filenames\n",
    "# but only one json file\n",
    "class EQDataSetJSON:\n",
    "    def __init__(self, filename_json: str = \"\"):\n",
    "        self.filename_json = filename_json\n",
    "        # Experiment definition file\n",
    "        with open(self.filename_json, 'r') as file:\n",
    "            self.json_data = json.load(file)\n",
    "        self.event_filenames = [d['event_filename'] for d in self.json_data]\n",
    "        self.stations = [d['station'] for d in self.json_data]\n",
    "        self.exclude_files = [d['exclude_file'] for d in self.json_data]\n",
    "        self.axis = [d['axis'] for d in self.json_data]\n",
    "        self.start_hours = [d['start_hour'] for d in self.json_data]\n",
    "        self.num_hours = [d['num_hours'] for d in self.json_data]\n",
    "        self.data = np.array([])\n",
    "        self.time_scale = []\n",
    "\n",
    "    # pull all the raw data from an experiment file\n",
    "    # Remove calibration events\n",
    "    # and concatenate into one long data vector\n",
    "    def get_data(self):\n",
    "        for i, event_filename in enumerate(self.event_filenames):\n",
    "            print(\"Reading in event file \" + event_filename)\n",
    "            with h5py.File(event_filename, 'r') as file:\n",
    "                station = self.stations[i]\n",
    "                exclude_file = self.exclude_files[i]\n",
    "                axis = self.axis[i]\n",
    "                print(\"Reading in axis \" + axis)\n",
    "                axis_dict = {'N': 0, 'E': 1, 'Z': 2}\n",
    "                axis = axis_dict[axis]\n",
    "                start_hour = self.start_hours[i]\n",
    "                num_hours = self.num_hours[i]\n",
    "                start = int(start_hour * 3600 * SAMPLING_RATE)\n",
    "                end = int((start_hour + num_hours) * 3600 * SAMPLING_RATE)\n",
    "                #print([f for f in file.items()])\n",
    "                #print(list(file.attrs))\n",
    "                for s in file.items():\n",
    "                    if s[0] == station:\n",
    "                        print(\"Reading in data for station \" + station)\n",
    "                        if num_hours:\n",
    "                            raw_data = s[1][axis, start:end]\n",
    "                        else:\n",
    "                            raw_data = s[1][axis, start:]\n",
    "                        #NOT USED\n",
    "                        if exclude_file:\n",
    "                            raw_data = EQDataSet.exclude_marked_up(raw_data, exclude_file=exclude_file)\n",
    "                        #plt.plot(raw_data)\n",
    "                        #plt.title(f\"Raw data PRE for {station}\")\n",
    "                        #plt.show()\n",
    "                        raw_data, time_scale = self.exclude_calibration_events(raw_data)\n",
    "                        #plt.plot(raw_data)\n",
    "                        #plt.ylabel(\"Amplitude\")\n",
    "                        #plt.xlabel(\"Sample number\")\n",
    "                        #plt.title(f\"Raw data POST for {station}\")\n",
    "                        #plt.show()\n",
    "                        #sys.exit()\n",
    "            self.data = np.append(self.data, raw_data)\n",
    "            print(\"New length of data: \" + str(len(self.data)) + \" samples\")\n",
    "            # this gives the approximate location of 9am JST\n",
    "            # each day\n",
    "            self.time_scale.append(time_scale)\n",
    "\n",
    "    # Used to find calibration events for Hi-net data\n",
    "    @staticmethod\n",
    "    def compare_activity_lag(raw_data, i, lag, radius):\n",
    "        return np.mean(raw_data[i-radius:i]) - np.mean(raw_data[i-radius-lag:i-lag])\n",
    "\n",
    "    def seek_calibration_event_grid(self, raw_data):\n",
    "        # given there are 31.25 days of data at 100Hz\n",
    "        # the first calibration event should occur in the first 24\n",
    "        # hours of data\n",
    "        # so we can look for the first calibration event  in the first\n",
    "        # 24*60*60*100 samples (1 day).\n",
    "        # Based on this there should also be a calibration event\n",
    "        # 5 seconds later\n",
    "        # and also 12 hours later and 12 hours 5 seconds later.\n",
    "        # We attempt to detect a calibration event by moving along\n",
    "        # sample by sample and looking for a sudden jump in the data\n",
    "        # that is greater than the previous jump by a factor of jump_factor (e.g. 2000)\n",
    "        # We then check for a similar jump 5 seconds later\n",
    "        # and if we find it we check for a similar jump 12 hours later\n",
    "        # and if we find it We then check for a similar jump 5 seconds later after that.\n",
    "        # If all of these are fulfilled, it is marked as a calibration event\n",
    "        one_day = 24*60*60*SAMPLING_RATE\n",
    "        five_seconds = 5*SAMPLING_RATE\n",
    "        # How far the mean data has to jump to be a candidate\n",
    "        # for a calibration event.\n",
    "        # approximated by experiment\n",
    "        jump_factor  = 4000 #4000 #2000 #50000\n",
    "        # how often we check for a calibration event.\n",
    "        # approximated by experiment\n",
    "        step_size = 1000 #2500 #7500\n",
    "        # how many samples we look back to calculate the mean.\n",
    "        # approximated by experiment\n",
    "        radius = 100 #100\n",
    "        # only need to confirm the first calibration event\n",
    "        # to have found them all.\n",
    "        first_calibration_index = -1\n",
    "        for i in range(step_size, one_day, step_size):\n",
    "            # check for a sudden jump in the data\n",
    "            if self.compare_activity_lag(raw_data,\n",
    "                                         i, step_size,\n",
    "                                         radius) > jump_factor:\n",
    "                # we have found a sudden jump in the data\n",
    "                # that is greater than the previous jump by a factor of 10\n",
    "                # check for a similar jump 5 seconds later\n",
    "                if self.compare_activity_lag(raw_data,\n",
    "                                         i+five_seconds,\n",
    "                                         step_size,\n",
    "                                         radius) > jump_factor:\n",
    "\n",
    "                    if self.compare_activity_lag(raw_data,\n",
    "                                                 i+one_day, step_size,\n",
    "                                                 radius) > jump_factor:\n",
    "                        # we have found a sudden jump in the data\n",
    "                        # that is greater than the previous jump by a factor of 10\n",
    "                        # check for a similar jump 5 seconds later\n",
    "                        if self.compare_activity_lag(raw_data,\n",
    "                                                     i + one_day+five_seconds,\n",
    "                                                     step_size,\n",
    "                                                     radius) > jump_factor:\n",
    "\n",
    "                            first_calibration_index = i+radius\n",
    "                            print(f\"Found calibration event at index {i}\")\n",
    "                            break\n",
    "        return first_calibration_index\n",
    "\n",
    "    def exclude_calibration_events(self, raw_data):\n",
    "        print(\"Seeking calibration events...\")\n",
    "        first_calibration_index = self.seek_calibration_event_grid(raw_data)\n",
    "        if first_calibration_index == -1:\n",
    "            return raw_data, None\n",
    "        # generate a mask for the 9am calibration events\n",
    "        # which will be seperated by 24*60*60*100 samples\n",
    "        calib_event_1_indices = np.arange(first_calibration_index,\n",
    "                                          len(raw_data), 24*60*60*SAMPLING_RATE)\n",
    "        print(f\"{calib_event_1_indices=}\")\n",
    "        exclude_radius = 15000 #15000 #15000\n",
    "        indices_to_delete = []\n",
    "        for cei in calib_event_1_indices:\n",
    "            # delete the vector elements from cei - exclude_radius to cei + exclude_radius\n",
    "            indices_to_delete += list(range(cei-exclude_radius, cei+exclude_radius))\n",
    "        total_time_removed = len(indices_to_delete)/SAMPLING_RATE\n",
    "        print(f\"Total time removed with calib events: {total_time_removed} seconds\")\n",
    "        raw_data = np.delete(raw_data, indices_to_delete)\n",
    "        time_scale = calib_event_1_indices-exclude_radius  # gives the approx 9am each day\n",
    "        time_scale= np.append(np.array([0]), time_scale)\n",
    "        time_scale = np.append(time_scale, np.array(len(raw_data)))\n",
    "        print(\"Calibration events deleted\")\n",
    "        return raw_data, time_scale\n",
    "\n",
    "    # This creates a sort of Franken\n",
    "    def generate_EQDataSetCombined(self, axis):\n",
    "        eqdsc = EQDataSetSingleSensorCombined(filename=self.filename_json,\n",
    "                                              axis=axis,\n",
    "                                              for_conversion=True)\n",
    "        # the two filenames refer to different types but should be ok\n",
    "        # other elements can be left alone hopefully\n",
    "        eqdsc.stations = self.stations  # stations from the json\n",
    "        # need to copy the concatenated data\n",
    "        # from the EQDataSetJSON\n",
    "        eqdsc.combined_data = self.data\n",
    "        eqdsc.time_scale = self.time_scale # list but should be ok\n",
    "        eqdsc.start_hour = self.start_hours # list but should be ok\n",
    "        eqdsc.num_hours = self.num_hours # list but should be ok\n",
    "        eqdsc.axis = axis  # list but should be ok\n",
    "        # eqdsc.magnitude  doesn't exist in json version\n",
    "        # eqdsc.time doesn't exist in json version\n",
    "        # eqdsc.only_include doesn't exist in json version\n",
    "        return eqdsc\n",
    "\n",
    "\n",
    "class EQDataSetProcessor:\n",
    "    def __init__(self, dataset: EQDataSetSingleSensorCombined, rebuild_features=True):\n",
    "        # the dataset contains a list of station data etc\n",
    "        self.dataset: EQDataSetSingleSensorCombined = dataset\n",
    "        self.rebuild_features: bool = rebuild_features\n",
    "        # short hand for the data set\n",
    "        self.ds: np.ndarray = self.dataset.combined_data\n",
    "        # data set after being high pass filtered (band pass?)\n",
    "        self.filtered_data: np.ndarray = np.array([])\n",
    "        # data after being chunked in 500s pre-feature analysis\n",
    "        self.chunked_data: np.ndarray = np.array([])\n",
    "        # data after each of the 500 samples being turned\n",
    "        # into a single set of 7 features\n",
    "        self.features_df: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "    def __str__(self):\n",
    "        ret_str = \"-\" * 30 + \"\\n\"\n",
    "        ret_str += \"Dataset Processor\\n\"\n",
    "        ret_str += \"-\" * 30 + \"\\n\"\n",
    "        ret_str += f\"Stations combined: {self.dataset.stations[:self.dataset.only_include]}\\n\"\n",
    "        ret_str += f\"Axis: {self.dataset.axis}\\n\"\n",
    "        ret_str += f\"Start: {self.dataset.start_hour}\\n\"\n",
    "        ret_str += f\"Num Hours: {self.dataset.num_hours}\\n\"\n",
    "        ret_str += f\"Dataset: {self.ds.shape}\\n\"\n",
    "        ret_str += f\"Filtered: {self.filtered_data.shape}\\n\"\n",
    "        ret_str += f\"Chunked: {self.chunked_data.shape}\\n\"\n",
    "        ret_str += f\"Features: {self.features_df.shape}\\n\"\n",
    "        return ret_str\n",
    "\n",
    "    # high pass filter\n",
    "    # has to be static so that it can be used in the map function\n",
    "    @staticmethod\n",
    "    def HPF(data: np.ndarray = None):\n",
    "        cutoff_frequency: float = 1  # Hz\n",
    "        # this function implements a one hz high pass filter\n",
    "        #b, a = butter(10, cutoff_frequency, btype='high',\n",
    "                      #fs=SAMPLING_RATE)\n",
    "        #b, a = butter(BUTTER_ORDER, [1,SAMPLING_RATE//2 - 1], btype='band',\n",
    "        #              fs=SAMPLING_RATE)\n",
    "        b, a = butter(BUTTER_ORDER, 1, btype='high',\n",
    "                                    fs=SAMPLING_RATE)\n",
    "        # The filtfilt function performs this bidirectional filtering by\n",
    "        # applying the filter coefficients both forwards and backwards.\n",
    "        # This process effectively eliminates the phase distortion\n",
    "        # introduced by the filter and provides a more accurate\n",
    "        # representation of the filtered signal in the time domain.\n",
    "        filtered_data = filtfilt(b, a, data)\n",
    "        #if random.random() < 0.01:\n",
    "        #    print(\".\", end=\"\")\n",
    "        return filtered_data\n",
    "\n",
    "    # non-static high pass filter\n",
    "    def apply_HPF(self, rebuild_filtered=True):\n",
    "        if not rebuild_filtered:\n",
    "            print(\"Loading in a presaved filtered data\")\n",
    "            with open('filtered_data.pkl', 'rb') as f:\n",
    "                self.filtered_data = pickle.load(f)\n",
    "            return\n",
    "        cutoff_frequency: float = 1  # Hz\n",
    "        # this function implements a one hz high pass filter\n",
    "        # Second order filter low pass Butterworth\n",
    "        # filter will have moderate roll-off (coz order 2)\n",
    "        # designed to cuttoff stuff above the Nyquist freq\n",
    "        # which is not well sampled.\n",
    "        # Designing a second-order Butterworth high-pass filter\n",
    "        # b and a are the filter parameters\n",
    "        # b, a = butter(10, cutoff_frequency, btype='high',\n",
    "        # fs=SAMPLING_RATE)\n",
    "        b, a = butter(BUTTER_ORDER, [1, SAMPLING_RATE // 2 - 1], btype='band',\n",
    "                      fs=SAMPLING_RATE)\n",
    "        # The filtfilt function performs this bidirectional filtering by\n",
    "        # applying the filter coefficients both forwards and backwards.\n",
    "        # This process effectively eliminates the phase distortion\n",
    "        # introduced by the filter and provides a more accurate\n",
    "        # representation of the filtered signal in the time domain.\n",
    "        self.filtered_data = filtfilt(b, a, self.ds)\n",
    "        with open('filtered_data.pkl', 'wb') as f:\n",
    "            pickle.dump(self.filtered_data, f)\n",
    "\n",
    "\n",
    "    def chunk_data_filtered(self):\n",
    "        # split the data into 500 sample chunks\n",
    "        # this is to make the data more manageable\n",
    "        # for the FFT\n",
    "        print(\"filtered data size is \", self.filtered_data.shape)\n",
    "        chunk_size = BLOCK_SIZE\n",
    "        # check if the data is a multiple of the chunk size\n",
    "        # if not, then drop the last few samples\n",
    "        if len(self.filtered_data) % chunk_size != 0:\n",
    "            self.filtered_data = self.filtered_data[:-(len(self.filtered_data) % chunk_size)]\n",
    "        num_chunks = len(self.filtered_data) // chunk_size\n",
    "        print(\"Chunking data\")\n",
    "        chunks = np.array_split(self.filtered_data, num_chunks)\n",
    "        # also split self.ds into chunks of the same size, leaving\n",
    "        # out any remainder\n",
    "\n",
    "        # convert to numpy array\n",
    "        chunks = np.array(chunks)\n",
    "\n",
    "        # convert back to numpy array\n",
    "        # but drop the last chunk assuming it is not the right size\n",
    "        #for c in chunks:\n",
    "            #print(f\"chunk shape is {c.shape}\")\n",
    "        print(\"BLOCK_SIZE is \", BLOCK_SIZE)\n",
    "        print(\"num chunks is \", num_chunks)\n",
    "        self.chunked_data = chunks\n",
    "\n",
    "    def p_integral_squared(self, X):\n",
    "        integral_squared = np.trapz(X ** 2, axis=1)\n",
    "        return integral_squared\n",
    "\n",
    "    def p_max_spectral_amp(self, Fx):\n",
    "        max_spectral_amp = np.max(np.abs(Fx), axis=1)  # this will be an array of size (N,)\n",
    "        return max_spectral_amp\n",
    "\n",
    "    def p_freq_at_max_spectral_amp(self, Fx, f):\n",
    "        print(\"freq at max spectral amp\")\n",
    "        freq_at_max_spectral_amp = f[np.argmax(Fx, axis=1)]\n",
    "        return freq_at_max_spectral_amp\n",
    "\n",
    "    def p_centre_freq(self, Fx, f):\n",
    "        centre_freq = np.sum(f[np.newaxis, :] * Fx, axis=1) / np.sum(Fx, axis=1)\n",
    "        signal_bandwidth = np.sqrt(\n",
    "            np.sum((f[np.newaxis, :] - centre_freq[:, np.newaxis]) ** 2 * Fx, axis=1) / np.sum(Fx, axis=1)\n",
    "        )\n",
    "        return centre_freq, signal_bandwidth\n",
    "\n",
    "    def p_zero_upcrossing_rate(self, Fx, f, w):\n",
    "        zero_upcrossing_rate = np.sqrt(\n",
    "            np.sum(w[np.newaxis, :] ** 2 * Fx ** 2, axis=1) / np.sum(Fx ** 2, axis=1)\n",
    "        )\n",
    "        return zero_upcrossing_rate\n",
    "\n",
    "    def p_rate_spectral_peaks(self, Fx, f, w):\n",
    "        rate_spectral_peaks = np.sqrt(\n",
    "            np.sum(w[np.newaxis, :] ** 4 * Fx ** 2, axis=1) / np.sum(w[np.newaxis, :] ** 2 * Fx ** 2, axis=1)\n",
    "        )\n",
    "        return rate_spectral_peaks\n",
    "\n",
    "    def compute_features(self):\n",
    "        # Compute FFT for each row\n",
    "        print(\"Computing features matrix\")\n",
    "        X = self.chunked_data\n",
    "        print(\"FFT\")\n",
    "        Fx = np.abs(np.fft.rfft(X, axis=1))\n",
    "        Fx = (2.0 / X.shape[1]) * Fx  # normalization\n",
    "        f = np.fft.rfftfreq(X.shape[1],\n",
    "                            1 / SAMPLING_RATE)  # Assuming self.dataset.sampling_rate is available\n",
    "        w = 2 * np.pi * f\n",
    "\n",
    "        # Compute features\n",
    "        #integral_squared = delta_t * np.sum(X**2, axis=1)\n",
    "        print(\"integral squared\")\n",
    "        integral_squared = np.trapz(X ** 2, axis=1)\n",
    "        print(\"max spectral amp\")\n",
    "        max_spectral_amp = np.max(np.abs(Fx), axis=1)  # this will be an array of size (N,)\n",
    "        print(\"freq at max spectral amp\")\n",
    "        freq_at_max_spectral_amp = f[np.argmax(Fx, axis=1)]\n",
    "        print(\"centre freq\")\n",
    "        centre_freq = np.sum(f[np.newaxis, :] * Fx, axis=1) / np.sum(Fx, axis=1)\n",
    "        signal_bandwidth = np.sqrt(\n",
    "            np.sum((f[np.newaxis, :] - centre_freq[:, np.newaxis]) ** 2 * Fx, axis=1) / np.sum(Fx, axis=1)\n",
    "        )\n",
    "        print(\"zero upcrossing rate\")\n",
    "        zero_upcrossing_rate = np.sqrt(\n",
    "            np.sum(w[np.newaxis, :] ** 2 * Fx ** 2, axis=1) / np.sum(Fx ** 2, axis=1)\n",
    "        )\n",
    "        print(\"rate spectral peaks\")\n",
    "        rate_spectral_peaks = np.sqrt(\n",
    "            np.sum(w[np.newaxis, :] ** 4 * Fx ** 2, axis=1) / np.sum(w[np.newaxis, :] ** 2 * Fx ** 2, axis=1)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Create DataFrame\n",
    "        features_df = pd.DataFrame({\n",
    "            'integral_squared': integral_squared,\n",
    "            'max_spectral_amp': max_spectral_amp,\n",
    "            'freq_at_max_spectral_amp': freq_at_max_spectral_amp,\n",
    "            'centre_freq': centre_freq,\n",
    "            'signal_bandwidth': signal_bandwidth,\n",
    "            'zero_upcrossing_rate': zero_upcrossing_rate,\n",
    "            'rate_spectral_peaks': rate_spectral_peaks\n",
    "        })\n",
    "        print(f\"features_df shape is {features_df.shape}\")\n",
    "        self.features_df = features_df\n",
    "\n",
    "    # core method of class\n",
    "    # it chunks the data into 500 sample chunks\n",
    "    # then applies the high pass filter to each chunk\n",
    "    # then computes the FFT for each chunk\n",
    "    # then computes the features for each chunk\n",
    "    # then returns a dataframe of the features\n",
    "    def compute_features_matrix(self, chunking=True, disable_filter=False, rebuild_hpf=True):\n",
    "        # only calculate features if not already saved to file\n",
    "        if self.rebuild_features or not os.path.exists('features.pkl'):\n",
    "            # apply hpf to whole dataset\n",
    "            if not disable_filter:\n",
    "                print(\"Applying high pass filter to whole dataset\")\n",
    "                self.apply_HPF(rebuild_hpf)\n",
    "\n",
    "            if chunking:\n",
    "                self.chunk_data_filtered()\n",
    "                print(f\"X shape is {self.chunked_data.shape}\")\n",
    "            else:\n",
    "                X = self.ds\n",
    "                print(f\"X Unchunked HPF shape is {X.shape}\")\n",
    "                self.chunked_data = X.reshape(1,BLOCK_SIZE)\n",
    "                print(f\"filtered ds rechunked HPF shape is {self.filtered_data.shape}\")\n",
    "            if disable_filter:\n",
    "                self.filtered_data = self.ds\n",
    "                self.chunk_data_filtered()\n",
    "            print(f\"filtered ds HPF shape is {self.filtered_data.shape}\")\n",
    "            self.compute_features() # note this works on filtered_data\n",
    "            # write features object to file using pickle\n",
    "            with open('features.pkl', 'wb') as f:\n",
    "                pickle.dump(self.features_df, f)\n",
    "        else:\n",
    "            # load features from file\n",
    "            print(\"Loading features from file\")\n",
    "            with open('features.pkl', 'rb') as f:\n",
    "                self.features_df = pickle.load(f)\n",
    "\n",
    "    def plot_features(self, percentage = 1.0):\n",
    "        print(\"Plotting features\")\n",
    "        # subplot the features in a graph above a subplot\n",
    "        # of the raw data so they are lined up\n",
    "        # plot the raw data\n",
    "        fig, axs = plt.subplots(\n",
    "            len(self.features_df.columns)+1, 1,\n",
    "            figsize=(15, 10))\n",
    "        axs[0].plot(self.ds[:int(percentage*len(self.ds))])\n",
    "        axs[0].set_title(f\"Raw data for {self.dataset.stations[:self.dataset.only_include]} {self.dataset.axis} \")\n",
    "        # create a dataframe of the features\n",
    "        # where each feature value is repeated BLOCK_SIZE times\n",
    "        # to line up with the raw data\n",
    "        lined_up_features_df = pd.DataFrame()\n",
    "        features_clipped = self.features_df[:int(percentage*len(self.features_df))]\n",
    "        for col in features_clipped.columns:\n",
    "            lined_up_features_df[col] = np.repeat(features_clipped[col].values, BLOCK_SIZE)\n",
    "\n",
    "        # plot the features each in their own subplot\n",
    "        for f in range(len(self.features_df.columns)):\n",
    "            axs[f+1].plot(\n",
    "                lined_up_features_df[\n",
    "                    self.features_df.columns[f]],\n",
    "                label=self.features_df.columns[f])\n",
    "            axs[f+1].set_title(f\"{self.features_df.columns[f]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# This class performs dimension reduction on the features\n",
    "# and searches for correct cluster count on train data\n",
    "# then clusters on the test data\n",
    "class EQDataSetClusterTrain:\n",
    "    def __init__(self, dataset_processed: EQDataSetProcessor, filename: str = ''):\n",
    "        if filename != '':\n",
    "            self.dataset_processed: EQDataSetProcessor = pickle.load(open(filename, 'rb'))\n",
    "        self.dataset_processed: EQDataSetProcessor = dataset_processed\n",
    "        self.test_features_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.train_features_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.standardised_train_features_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.standardised_test_features_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.test_size: float = 0.1\n",
    "        self.features_df = train_test_split(\n",
    "            self.dataset_processed.features_df,\n",
    "            test_size=self.test_size, shuffle=True) #, random_state=7)\n",
    "        self.features_df = {'train': self.features_df[0],\n",
    "                            'test': self.features_df[1]}\n",
    "        print(f\"{self.features_df=}\")\n",
    "        self.standardised_features_df: dict[str, pd.DataFrame] = {'train': pd.DataFrame(),\n",
    "                                                'test': pd.DataFrame()}\n",
    "        self.pca_df: dict[str, pd.DataFrame] = {'train': pd.DataFrame(),\n",
    "                                                'test': pd.DataFrame()}\n",
    "        self.explained_variance: dict[str, np.ndarray] = {'train': np.array([]),\n",
    "                                                'test': np.array([])}\n",
    "        self.features_pca: dict[str, PCA] = {'train':\n",
    "                                                 PCA(whiten=True),\n",
    "                                             'test':\n",
    "                                                 PCA(whiten=True)\n",
    "                                        }\n",
    "        self.max_clusters: int = 10\n",
    "        self.train_clusters: KMeans = KMeans(n_clusters=self.max_clusters) #,\n",
    "                                            #random_state=7)\n",
    "        self.n_refs: int = 100  # number of references for gap score\n",
    "        self.gaps: list[np.ndarray] = list(np.array([]))\n",
    "        self.gaps_sd: list[np.ndarray] = list(np.array([]))\n",
    "        self.silhouette_scores: list[float] = list([])\n",
    "        self.gap = np.array([])\n",
    "        self.gap_sd = np.array([])\n",
    "        self.inertias = np.array([])\n",
    "        self.inertias_sd = np.array([])\n",
    "        self.silhouettes = np.array([])\n",
    "        self.silhouettes_sd = np.array([])\n",
    "\n",
    "    def __str__(self):\n",
    "        # use ret_str to build up the string to return\n",
    "        # all properties of the class are added to the string\n",
    "        ret_str = f\"EQDataSetClusterer object\\n\"\n",
    "        ret_str += f\"dataset_processed: {self.dataset_processed}\\n\"\n",
    "        ret_str += f\"test_features_df: {self.test_features_df}\\n\"\n",
    "        ret_str += f\"train_features_df: {self.train_features_df}\\n\"\n",
    "        ret_str += f\"standardised_train_features_df: {self.standardised_train_features_df}\\n\"\n",
    "        ret_str += f\"standardised_test_features_df: {self.standardised_test_features_df}\\n\"\n",
    "        ret_str += f\"test_size: {self.test_size}\\n\"\n",
    "        ret_str += f\"features_df: {self.features_df}\\n\"\n",
    "        ret_str += f\"standardised_features_df: {self.standardised_features_df}\\n\"\n",
    "        ret_str += f\"pca_df: {self.pca_df}\\n\"\n",
    "        ret_str += f\"explained_variance: {self.explained_variance}\\n\"\n",
    "        #ret_str += f\"n_components: {self.n_components}\\n\"\n",
    "        ret_str += f\"features_pca: {self.features_pca}\\n\"\n",
    "        ret_str += f\"max_clusters: {self.max_clusters}\\n\"\n",
    "        ret_str += f\"train_clusters: {self.train_clusters}\\n\"\n",
    "        ret_str += f\"n_refs: {self.n_refs}\\n\"\n",
    "        ret_str += f\"gaps: {self.gaps}\\n\"\n",
    "        ret_str += f\"gaps_sd: {self.gaps_sd}\\n\"\n",
    "        ret_str += f\"silhouette_scores: {self.silhouette_scores}\\n\"\n",
    "        ret_str += f\"gap: {self.gap}\\n\"\n",
    "        ret_str += f\"gap_sd: {self.gap_sd}\\n\"\n",
    "        return ret_str\n",
    "\n",
    "\n",
    "    # for each feature in the train and test df, subtract its mean and divide by its standard deviation\n",
    "    def standardise_features(self):\n",
    "        print(\"Standardising features\")\n",
    "        for d in ['train', 'test']:\n",
    "            self.standardised_features_df[d] = (self.features_df[d] -\n",
    "                                        self.features_df[d].mean()\n",
    "                                      ) / self.features_df[d].std()\n",
    "    # apply incremental PCA across the features\n",
    "    # and display percentage of variance explained\n",
    "    # by each component\n",
    "    def whitened_pca(self, trim_components: bool = True):\n",
    "        columns = [f\"PCA{i}\" for i in range(self.standardised_features_df['train'].shape[1])]\n",
    "        for d in ['train', 'test']:\n",
    "            print(f\"Whitening PCA for {d} data\")\n",
    "            pca_data = self.features_pca[d].fit_transform(self.standardised_features_df[d])\n",
    "            self.explained_variance[d] = self.features_pca[d].explained_variance_ratio_\n",
    "            print(f\"{d} explained variance is {self.explained_variance[d]}\")\n",
    "           \n",
    "            self.pca_df[d] = pd.DataFrame(pca_data, columns=columns, index=self.standardised_features_df[d].index)\n",
    "\n",
    "        if trim_components:\n",
    "            # go through pca until >95% of variance is explained\n",
    "            # and remove any components that don't explain much variance\n",
    "            var_sum = 0\n",
    "            n_components = 1\n",
    "            for v in self.explained_variance['train']:\n",
    "                var_sum += v\n",
    "                print(f\"% variance explained by {n_components} components is {var_sum}\")\n",
    "                if round(var_sum, 2) < 0.99: #0.9 25:\n",
    "                    n_components += 1\n",
    "                else:\n",
    "                    break\n",
    "            self.pca_df['train'] = self.pca_df['train'].iloc[:, :n_components]\n",
    "            self.pca_df['test'] = self.pca_df['test'].iloc[:, :n_components]\n",
    "\n",
    "    # new code as of 13 aug 2023\n",
    "    def gap_statistic(self, data, kmeans_object):\n",
    "        # Calculate the actual clustering's sum of squared distances\n",
    "        centroids = kmeans_object.cluster_centers_\n",
    "        # centroids.shape is (n_clusters, n_features) - e.g. (2, 6)\n",
    "        # for 2 clusters and the 6 PCA features\n",
    "        \"\"\"The inertia_ attribute of a KMeans object gives the sum of squared \n",
    "        distances of samples to their closest cluster center, which is \n",
    "        exactly what you're trying to compute.\n",
    "        \"\"\"\n",
    "        ssd_actual = kmeans_object.inertia_\n",
    "        n_refs = 100\n",
    "        ssd_refs = []\n",
    "        d = np.array(data)\n",
    "        data_mean = np.mean(d, axis=0).reshape(1, -1)\n",
    "        data_std = np.std(d, axis=0).reshape(1, -1)\n",
    "        for _ in range(n_refs):\n",
    "            random_data = (np.random.rand(*data.shape, ))\n",
    "            # scale random_data so it goes between max and min of data columns\n",
    "            random_data = random_data * (np.max(d, axis=0) - np.min(d, axis=0)) + np.min(d, axis=0)\n",
    "            km_ref = KMeans(n_clusters=len(centroids), n_init=1,\n",
    "                            init='random').fit(random_data)\n",
    "            # note km_ref.inertia_ is the sum of squared distances of fake\n",
    "            # samples to their closest cluster center\n",
    "            # which is exactly what you're trying to compute in the gap statistic\n",
    "            ssd_refs.append(km_ref.inertia_)\n",
    "        # Calculate the Gap statistic\n",
    "        gap_stat = np.mean(np.log(ssd_refs) - np.log(ssd_actual))\n",
    "        # Calculate the standard deviation of the Gap statistic\n",
    "        gaps_sd = np.std(np.log(ssd_refs)- np.log(ssd_actual))\n",
    "        ss = silhouette_score(data,\n",
    "                kmeans_object.labels_)\n",
    "        self.silhouette_scores.append(ss)\n",
    "        return gap_stat, gaps_sd\n",
    "\n",
    "    def run_training(self):\n",
    "        #print(self)\n",
    "        # display available memory\n",
    "        \"\"\"print(f\"Available memory pre delete: {psutil.virtual_memory()}\")\n",
    "        self.dataset_processed.dataset.combined_data = np.array([])\n",
    "        self.dataset_processed.filtered_data = np.array([])\n",
    "        self.dataset_processed.chunked_data = np.array([])\n",
    "        gc.collect()\n",
    "        print(f\"Available memory post delete: {psutil.virtual_memory()}\")\"\"\"\n",
    "        self.standardise_features()\n",
    "        self.whitened_pca()\n",
    "        # plot the pca correlation matrix\n",
    "        corr = self.pca_df['train'].corr()\n",
    "        plt.matshow(corr)\n",
    "        plt.title('PCA covariance matrix')\n",
    "        # colormap legend\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "        PROCESS_REPEAT_L0_N = 3  # 3\n",
    "        SAMPLE_REPEAT_L1_N = 52 #52 #5 #52 #80 #80//2 #<- for 4 stations #80//4 # <- for one station  # 500 in original\n",
    "        SAMPLE_SIZE =  7500 #15000  # 15000\n",
    "        MAX_K = 10 #10\n",
    "        USE_TQDM = False\n",
    "        if USE_TQDM:\n",
    "            process_repeat_range = tqdm(range(PROCESS_REPEAT_L0_N), desc=\"process_repeat\",\n",
    "                                        leave=True, position=0)\n",
    "        else:\n",
    "            process_repeat_range = range(PROCESS_REPEAT_L0_N)\n",
    "        for process_repeat in process_repeat_range: #tqdm(range(PROCESS_REPEAT_L0_N), desc=\"process_repeat\"): #range(PROCESS_REPEAT_L0_N):\n",
    "            if not USE_TQDM:\n",
    "                print(\"process_repeat:\", process_repeat)\n",
    "            # create a dictionary of lists to store the inertias for each k\n",
    "            all_inertias = {str(k): [] for k in range(2, MAX_K + 1)}\n",
    "            all_gaps = {str(k): [] for k in range(2, MAX_K + 1)}\n",
    "            all_gaps_sd = {str(k): [] for k in range(2, MAX_K + 1)}\n",
    "            all_silhouettes = {str(k): [] for k in range(2, MAX_K + 1)}\n",
    "            if USE_TQDM:\n",
    "                loop_obj_sample = tqdm(range(SAMPLE_REPEAT_L1_N), desc=\"\\tsample_repeat\",\n",
    "                                       leave=True, position=1)\n",
    "            else:\n",
    "                loop_obj_sample = range(SAMPLE_REPEAT_L1_N)\n",
    "            for sample_repeat in loop_obj_sample: #tqdm(range(SAMPLE_REPEAT_L1_N), desc=\"sample_repeat\"): #range(SAMPLE_REPEAT_L1_N):\n",
    "                if not USE_TQDM:\n",
    "                    print(\"\\tsample_repeat:\", sample_repeat)\n",
    "                # get a random sample of 15,000 from data\n",
    "                sample_df = self.pca_df['train'].sample(n=SAMPLE_SIZE)\n",
    "                if USE_TQDM:\n",
    "                    loop_obj_kmeans = tqdm(range(2, MAX_K + 1),\n",
    "                                           desc=\"\\t\\tkmeans\",\n",
    "                                           leave=True, position=2)\n",
    "                else:\n",
    "                    loop_obj_kmeans = range(2, MAX_K + 1)\n",
    "                for k in loop_obj_kmeans:\n",
    "                    if not USE_TQDM:\n",
    "                        print(\"\\t\\tk:\", k)\n",
    "                    run_kmeans = KMeans(\n",
    "                        n_clusters=k, n_init='auto',\n",
    "                        init='k-means++').fit(sample_df)\n",
    "                    gap_stat, gap_sd = self.gap_statistic(sample_df, run_kmeans)\n",
    "                    all_gaps[str(k)].append(gap_stat)\n",
    "                    all_gaps_sd[str(k)].append(gap_sd)\n",
    "                    # calculate the gap statistic using sample_store[min_index]\n",
    "                    # and store in all_gaps[str(k)]\n",
    "                    all_inertias[str(k)].append(run_kmeans.inertia_)\n",
    "                    all_silhouettes[str(k)].append(silhouette_score(sample_df,\n",
    "                        run_kmeans.labels_))\n",
    "                   \n",
    "            # Calculate the mean and standard deviation of the inertias for each k\n",
    "            print(\"Total min values collected from samples:\", len(all_inertias['2']))\n",
    "            mean_inertias = {k: round(np.mean(all_inertias[k]), 0) for k in all_inertias}\n",
    "            std_inertias = {k: round(np.std(all_inertias[k]), 0) for k in all_inertias}\n",
    "            mean_silhouettes = {k: np.mean(all_silhouettes[k]) for k in all_silhouettes}\n",
    "            std_silhouettes = {k: np.std(all_silhouettes[k]) for k in all_silhouettes}\n",
    "            mean_gap = {k: np.mean(all_gaps[k]) for k in all_gaps}\n",
    "            # note, the std of the gap statistic is not\n",
    "            # calculated, but is stored as the std of the reference\n",
    "            # for the error bars.\n",
    "            std_gap = {k: np.mean(all_gaps_sd[k]) for k in all_gaps_sd}\n",
    "            self.gap = np.append(self.gap,mean_gap)\n",
    "            self.gap_sd = np.append(self.gap_sd, std_gap)\n",
    "            self.inertias = np.append(self.inertias, mean_inertias)\n",
    "            self.inertias_sd = np.append(self.inertias_sd, std_inertias)\n",
    "            self.silhouettes = np.append(self.silhouettes, mean_silhouettes)\n",
    "            self.silhouettes_sd = np.append(self.silhouettes_sd, std_silhouettes)\n",
    "            self.plot_train_cluster_results(max_k=MAX_K, title=f\"Repeat {process_repeat+1}\")\n",
    "\n",
    "\n",
    "    def plot_train_cluster_results(self, max_k, title='Mean Gap stat plot', process_index: int=-1):\n",
    "        # plot the gap statistic across all training\n",
    "        # by default displays the most recent\n",
    "        # process_index = -1\n",
    "        # otherwise specify the index of the process to plot (usually 0 to 2)\n",
    "        gap = list(self.gap[process_index].values())\n",
    "        sd = list(self.gap_sd[process_index].values())\n",
    "        #sd = [s*1.96 for s in sd]\n",
    "        inertias = np.array(list(self.inertias[process_index].values()))\n",
    "        silhouettes = np.array(list(self.silhouettes[process_index].values()))\n",
    "\n",
    "        clusters = range(2, max_k + 1)\n",
    "        print(clusters)\n",
    "        print(gap)\n",
    "        #fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "        plt.errorbar(clusters, gap, yerr=sd)\n",
    "        # plt.plot(clusters, gap, label='gap statistic')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Gap statistic')\n",
    "        plt.title(f\"{title}: Gap statistic\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        # plot the derivative of the gapstatistic\n",
    "\n",
    "        print(\"np.diff(gap): \",np.diff(gap)/sd[-1])\n",
    "        #plt.plot(clusters[1:], np.diff(gap) / sd[:-1], color='r', label='derivative of gap statistic')\n",
    "        plt.plot(clusters[1:], np.diff(gap)/sd[-1], color='r', label='derivative of gap statistic')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Derivative of Gap statistic')\n",
    "        plt.title(f\"{title}: Deriv. gap statistic\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        # plot inertia scores\n",
    "        plt.plot(clusters, inertias, color='g', label='inertia')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.title(f\"{title}: Inertia\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        # plot silhouette scores\n",
    "\n",
    "        plt.plot(clusters, silhouettes, color='g', label='inertia')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Silhouette')\n",
    "        plt.title(f\"{title}: Silhouette\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    def load_training(self):\n",
    "        # load training and test data from file\n",
    "        with open('train_features.pkl', 'rb') as f:\n",
    "            train_features = pickle.load(f)\n",
    "        with open('test_features.pkl', 'rb') as f:\n",
    "            test_features = pickle.load(f)\n",
    "\n",
    "\n",
    "# This class uses the results of EQDataSetClusterTrain to cluster the Test dataset\n",
    "class EQDataSetClusterTest:\n",
    "    def __init__(self, training_structure: EQDataSetClusterTrain, num_clusters=5):\n",
    "        # this is the training structure that would've been used to decide on\n",
    "        # the number of clusters\n",
    "        self.training_structure: EQDataSetClusterTrain = training_structure\n",
    "        # this is the number of clusters that was decided on\n",
    "        self.num_clusters: int = num_clusters\n",
    "        self.sample_size: int = 8000 #not used\n",
    "        self.final_kmeans = None\n",
    "        self.n_init: str = 'auto'\n",
    "        self.max_no_improvement: int = 250 #500\n",
    "        # The results of compute_initial_centroids are stored here:\n",
    "        self.initial_centroids = None\n",
    "        self.labels = None\n",
    "        self.raw_data_with_clusters: pd.DataFrame = pd.DataFrame()\n",
    "        self.first_raw_row_index: int = 0\n",
    "        self.all_clusters = None\n",
    "        # these are used to enable clean reversing\n",
    "        self.labels_train = None\n",
    "\n",
    "    # this uses the algorithm from the Johnson paper\n",
    "    def compute_initial_centroids(self):\n",
    "        print(\"Computing initial centroids\")\n",
    "        data = self.training_structure.pca_df['test']\n",
    "        n_clusters = self.num_clusters\n",
    "        #print(f\"len(data) = {len(data)}\")\n",
    "        print(f\"sample_size = {self.sample_size} of {len(data)}\")\n",
    "        best_inertia = float('inf')\n",
    "        no_improvement_count: int = 0\n",
    "        best_centroids = None\n",
    "\n",
    "        while no_improvement_count < self.max_no_improvement:\n",
    "            if no_improvement_count % 50 == 0:\n",
    "                print(f\"no_improvement_count = {no_improvement_count}\")\n",
    "            # Sample a batch from the data\n",
    "            indices = np.random.choice(data.shape[0],\n",
    "                                       self.sample_size, replace=False)\n",
    "            # print(f\"indices = {indices}\")\n",
    "            sample = data.iloc[indices]\n",
    "            # print(f\"batch.shape = {batch.shape}\")\n",
    "            # Perform KMeans clustering on the batch\n",
    "            kmeans_results = KMeans(n_clusters=n_clusters,\n",
    "                                    init='random',\n",
    "                                    n_init=self.n_init).fit(sample)\n",
    "            # Check for improvement\n",
    "            if kmeans_results.inertia_ < best_inertia:\n",
    "\n",
    "                best_inertia = kmeans_results.inertia_\n",
    "                best_centroids = kmeans_results.cluster_centers_\n",
    "                print(f\"New best inertia {best_inertia} and centroids {best_centroids}\")\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "        self.initial_centroids = best_centroids\n",
    "\n",
    "    def cluster_test(self):\n",
    "        # final model is calculated over the test data using 5 clusters.\n",
    "        # initial centroids are used to seed the k-means model for optimisation\n",
    "        # using all evaluation data and apply a cluster label to each.\n",
    "\n",
    "        self.sample_size: int = 25000 #8000\n",
    "        self.n_init = 100 #80 #500 #80\n",
    "        self.max_no_improvement: int = 500 #100 #80 #250 #    500 #250  # 500\n",
    "\n",
    "        REBUILD_CENTROIDS = True\n",
    "        # check if initial centroids have already been computed\n",
    "        if not REBUILD_CENTROIDS and os.path.exists('initial_centroids.pkl'):\n",
    "            # load initial centroids from file\n",
    "            with open('initial_centroids.pkl', 'rb') as f:\n",
    "                self.initial_centroids = pickle.load(f)\n",
    "        else:\n",
    "            self.compute_initial_centroids()\n",
    "            # pickle initial centroids\n",
    "            with open('initial_centroids.pkl', 'wb') as f:\n",
    "                pickle.dump(self.initial_centroids, f)\n",
    "\n",
    "        # Use the initial centroids to seed the KMeans model and fit on all data\n",
    "        print(\"Running full k-means test\")\n",
    "        self.final_kmeans = KMeans(n_clusters=self.num_clusters,\n",
    "                              init=self.initial_centroids).fit(\n",
    "                                self.training_structure.pca_df['test'])\n",
    "        # Assign cluster labels to each data point\n",
    "        self.labels = self.final_kmeans.labels_\n",
    "\n",
    "        # Count occurrences of each label\n",
    "        label_counts = np.bincount(self.labels)\n",
    "\n",
    "        # Compute percentages\n",
    "        label_percentages = (label_counts / len(self.labels)) * 100\n",
    "\n",
    "        # Output percentages for each label\n",
    "        for i, percentage in enumerate(label_percentages):\n",
    "            print(f\"Label {i}: {percentage:.2f}%\")\n",
    "\n",
    "        print(\"Running full k-means train\")\n",
    "        self.final_kmeans_train = KMeans(n_clusters=self.num_clusters,\n",
    "                                   init=self.initial_centroids).fit(\n",
    "            self.training_structure.pca_df['train'])\n",
    "        # Assign cluster labels to each data point\n",
    "        self.labels_train = self.final_kmeans_train.labels_\n",
    "\n",
    "\n",
    "    def get_original_cluster_feature_rows(self):\n",
    "        # map the original test feature rows to the cluster labels\n",
    "        # this is the data that was used to train the model\n",
    "        # after it was chunked but before it was standardized and reduced to 6 dimensions\n",
    "        clusters = []\n",
    "        print(f\"{self.training_structure.features_df['train']=}\")\n",
    "        print(f\"{self.training_structure.features_df['test']=}\")\n",
    "        for i in range(self.num_clusters):\n",
    "            clusters.append(self.training_structure.features_df['test'][self.labels == i])\n",
    "        # create a train clusters dataframe which consists of the original train feature row\n",
    "        # indices and a column of \"fake\" clusters labels which are all -1\n",
    "        train_clusters = pd.DataFrame(\n",
    "            {'orig_index': self.training_structure.features_df['train'].index,\n",
    "             'cluster': self.labels_train})\n",
    "\n",
    "        # create a test clusters dataframe which consists of the original test features row\n",
    "        # indices and a column of the relevant cluster labels calculated in the loop above\n",
    "        test_clusters = pd.DataFrame(\n",
    "            {'orig_index': self.training_structure.features_df['test'].index,\n",
    "             'cluster': self.labels})\n",
    "        #print(self.training_structure.features_df['test'])\n",
    "        #print(self.training_structure.features_df['train'])\n",
    "        \n",
    "        # now combine the above two dataframes into a single dataframe\n",
    "        self.all_clusters = pd.concat([train_clusters, test_clusters])\n",
    "\n",
    "        # now sort the dataframe by the index column\n",
    "        self.all_clusters.sort_values(by='orig_index', inplace=True)\n",
    "        print(f\"{self.all_clusters=}\")\n",
    "        \n",
    "        self.training_structure.features_df['test']['cluster'] = test_clusters['cluster'].values\n",
    "        self.training_structure.features_df['train']['cluster'] = train_clusters['cluster'].values\n",
    "\n",
    "\n",
    "    # function to use the self.training_structure.features_df['test']['cluster'] column\n",
    "    # to map the original raw test data rows to the cluster labels\n",
    "    def get_original_cluster_raw_rows_no_shuffle(self):\n",
    "        # get the original raw data from the dataset\n",
    "        # which was turned into the features that become the test feature data\n",
    "        # assume unshuffled data\n",
    "        # get first row index of test feature data\n",
    "        first_test_row_index = self.training_structure.features_df['test'].index[0]\n",
    "        #print(first_test_row_index)\n",
    "        # convert it to the first row index of the raw data, given that the raw\n",
    "        # data is unshuffled and BLOCK_SIZE rows are used to create each feature row\n",
    "        self.first_raw_row_index = first_test_row_index * BLOCK_SIZE\n",
    "        print(f\"{self.first_raw_row_index=}\")\n",
    "        raw_test_data = self.training_structure.dataset_processed.dataset.combined_data[self.first_raw_row_index:]\n",
    "        print(raw_test_data)\n",
    "        # create a dataframe of the features\n",
    "        # where each feature value is repeated BLOCK_SIZE times\n",
    "        # to line up with the raw data\n",
    "        lined_up_raw_data_df = pd.DataFrame()\n",
    "        lined_up_raw_data_df['orig_data'] = raw_test_data\n",
    "        for col in self.training_structure.features_df['test'].columns:\n",
    "            # if the remaining rows are a multiple of BLOCK_SIZE\n",
    "            if len(raw_test_data) % BLOCK_SIZE == 0:\n",
    "                lined_up_raw_data_df[col] = np.repeat(\n",
    "                    self.training_structure.features_df['test'][col].values, BLOCK_SIZE)\n",
    "            else:\n",
    "                even_rows = np.repeat(\n",
    "                    self.training_structure.features_df['test'][col].values, BLOCK_SIZE)\n",
    "                leftover_rows = np.array([0] * (len(raw_test_data) % BLOCK_SIZE))\n",
    "                \n",
    "                lined_up_raw_data_df[col] = np.concatenate((even_rows, leftover_rows))\n",
    "                # fill in the last part (which is less than BLOCK_SIZE) with the last value\n",
    "        # add the cluster labels to the dataframe\n",
    "        self.raw_data_with_clusters = lined_up_raw_data_df\n",
    "\n",
    "\n",
    "# This class uses the results of EQDataSetClusterTest to plot graphs\n",
    "class EQDataSetClusterTestPlot:\n",
    "    def __init__(self, test_structure: EQDataSetClusterTest):\n",
    "        self.tst_st: EQDataSetClusterTest = test_structure\n",
    "\n",
    "    def plot_features_and_clusters_non_shuffled(self, proportion=1.0):\n",
    "        # subplot the features in a graph above a subplot\n",
    "        # of the raw data so they are lined up\n",
    "        # plot the raw data\n",
    "        end_raw = int(len(self.tst_st.raw_data_with_clusters) * proportion)\n",
    "        orig_features = self.tst_st.training_structure.dataset_processed.features_df\n",
    "        orig_columns = orig_features.columns\n",
    "        num_features = len(orig_features.columns)\n",
    "        clusters = self.tst_st.raw_data_with_clusters['cluster'].values\n",
    "        fig, axs = plt.subplots(\n",
    "            num_features+2, 1,\n",
    "            figsize=(15, 10))\n",
    "        orig_data = self.tst_st.training_structure.dataset_processed.dataset.combined_data[self.tst_st.first_raw_row_index:]\n",
    "        axs[0].plot(orig_data[:end_raw])\n",
    "        stations = self.tst_st.training_structure.dataset_processed.dataset.stations[:self.tst_st.training_structure.dataset_processed.dataset.only_include]\n",
    "        axs[0].set_title(\n",
    "            f\"Raw data for {stations} {self.tst_st.training_structure.dataset_processed.dataset.axis} \")\n",
    "            #f\"Raw data for {self.tst_st.training_structure.dataset_processed.dataset.stations} {self.tst_st.training_structure.dataset_processed.dataset.axis} \")\n",
    "        axs[1].plot(clusters[:end_raw]+1)\n",
    "        axs[1].set_title(\n",
    "            f\"Clusters for {self.tst_st.training_structure.dataset_processed.dataset.stations} {self.tst_st.training_structure.dataset_processed.dataset.axis} \")\n",
    "        # plot the features each in their own subplot\n",
    "        for f in range(num_features):\n",
    "            axs[f+2].plot(\n",
    "                self.tst_st.raw_data_with_clusters[orig_columns[f]].iloc[:end_raw],\n",
    "                label=orig_columns[f])\n",
    "            axs[f+2].set_title(f\"{orig_columns[f]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_raw_data_with_cluster_colours(self, proportion:float = 1):\n",
    "        end = int(self.tst_st.training_structure.dataset_processed.filtered_data.shape[0] * proportion)\n",
    "        end = int(self.tst_st.training_structure.dataset_processed.dataset.combined_data.shape[0] * proportion)\n",
    "        # Define a list of colors (expand this list if you have more clusters)\n",
    "        colors = ['red', 'blue', 'green', 'black', 'purple','orange', 'yellow', 'pink', 'brown', 'grey']\n",
    "        # Plot each segment colored by its cluster\n",
    "        current_chunk_index = 0\n",
    "        current_sub_counter = 0\n",
    "        for i in range(1, end):\n",
    "            xy = [self.tst_st.training_structure.dataset_processed.dataset.combined_data[i - 1],\n",
    "                  self.tst_st.training_structure.dataset_processed.dataset.combined_data[i]]\n",
    "            plt.plot([i - 1, i], xy , \n",
    "                      color = colors[int(self.tst_st.all_clusters.iloc[current_chunk_index,-1])])\n",
    "\n",
    "            current_sub_counter += 1\n",
    "            if current_sub_counter == BLOCK_SIZE:\n",
    "                current_sub_counter = 0\n",
    "                current_chunk_index += 1\n",
    "        plt.xlabel('time (100ths of a second)')\n",
    "        plt.ylabel('Seismic Amplitude')\n",
    "        # make the title fit into the graph\n",
    "        plot_title ='Samples Colored by Cluster Assignment \\n'\n",
    "        num_stations = len(self.tst_st.training_structure.dataset_processed.dataset.stations)\n",
    "        if num_stations > 7:\n",
    "            plot_title += f\"for {num_stations} stations with {self.tst_st.training_structure.dataset_processed.dataset.axis}\"\n",
    "        else:\n",
    "            plot_title += f'{self.tst_st.training_structure.dataset_processed.dataset.stations} {self.tst_st.training_structure.dataset_processed.dataset.axis}'\n",
    "\n",
    "        plt.title(plot_title)\n",
    "        # create a legend which maps from cluster number to color\n",
    "        legend_elements = []\n",
    "        for c in range(self.tst_st.num_clusters):\n",
    "            legend_elements.append(Line2D([0], [0], color=colors[c], lw=4, label=f'Cluster {c}'))\n",
    "        \n",
    "        plt.legend(handles=legend_elements)\n",
    "        # put the legend outside the plot\n",
    "        plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        # resize the plot to fit the legend\n",
    "        plt.subplots_adjust(right=0.7)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_station_data(station_name='IWEH', filename='all_stations.json'):\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for d in data:\n",
    "            if d['name'][:4] == station_name:\n",
    "                return d\n",
    "        return None\n",
    "\n",
    "    def drum_plot_with_cluster_colours(self, proportion:float = 1):\n",
    "        # Define a list of colors (expand this list if you have more clusters)\n",
    "        colors = ['red', 'blue', 'green', 'black', 'purple', 'orange', 'yellow', 'pink', 'brown', 'grey']\n",
    "\n",
    "        num_stations = 10\n",
    "\n",
    "        A = self.tst_st.training_structure.dataset_processed.dataset.time_scale #eqdsp.dataset.time_scale\n",
    "        B = self.tst_st.training_structure.dataset_processed.dataset.combined_data\n",
    "        C = self.tst_st.all_clusters.iloc[:, -1].to_numpy()\n",
    "        print(f\"{C=}\")\n",
    "\n",
    "        # for a 48 hour set\n",
    "        # A[i][1] is location of 9am-ish\n",
    "        # A [i][3] is the end of the data\n",
    "        # So location of next 9am-ish is 1*A[i][3] + A[i+1][1]\n",
    "        # So location of next 9am-ish is 2*A[i+1][3] + A[i+2][1]\n",
    "        # or looking into past\n",
    "        # So location of next 9am-ish is i*A[i-1][3] + A[i][1]\n",
    "        print(f\"{A[:num_stations]=}\")\n",
    "        old_A = A\n",
    "        new_A = [0 * 0 + old_A[0][1]]\n",
    "        for i in range(1, len(A[:num_stations])):\n",
    "            new_A.append(i * old_A[i - 1][3] + old_A[i][1] + 1)\n",
    "        A = new_A\n",
    "\n",
    "        print(f\"A={A}\")\n",
    "        #B = eqdsp.dataset.combined_data\n",
    "        # Extract sub-vectors\n",
    "        num_mins = 15\n",
    "        subv_len = 60 * SAMPLING_RATE * num_mins\n",
    "\n",
    "        divider = 250\n",
    "\n",
    "        sub_vectors = [B[start:start + subv_len] / divider for start in A]\n",
    "        sub_vectors_chunks = [C[math.floor(start/BLOCK_SIZE):math.floor((start + subv_len)/BLOCK_SIZE)]  for start in A]\n",
    "       \n",
    "        print(f\"sub_vectors_chunks={sub_vectors_chunks}\")\n",
    "\n",
    "        # Plot sub-vectors vertically next to each other\n",
    "        plt.figure(figsize=(10, 8))\n",
    "       \n",
    "\n",
    "        for idx, sub_vector in enumerate(sub_vectors):\n",
    "            # Plot each segment colored by its cluster\n",
    "            current_chunk_index = 0\n",
    "            current_sub_counter = 0\n",
    "            divider = 1\n",
    "            for i in range(1, subv_len):\n",
    "                xy = [sub_vector[i - 1],\n",
    "                      sub_vector[i]]\n",
    "                # print(f\"{xy=}\")\n",
    "                xy  = [z/divider+ idx * 5 for z in xy]  #\n",
    "                #print(sub_vectors_chunks[current_chunk_index])\n",
    "                plt.plot(xy, [i - 1, i],  \n",
    "                         color=colors[int(sub_vectors_chunks[idx][current_chunk_index])])\n",
    "\n",
    "                current_sub_counter += 1\n",
    "                if current_sub_counter == 500:\n",
    "                    current_sub_counter = 0\n",
    "                    current_chunk_index += 1\n",
    "\n",
    "        stations= self.tst_st.training_structure.dataset_processed.dataset.stations[:num_stations]\n",
    "        print(stations)\n",
    "        custom_labels = []\n",
    "        for idx, station in enumerate(stations):\n",
    "            custom_labels.append(f\"{station}\\n({round(self.get_station_data(station)['latitude'], 1)})\")\n",
    "        #print(eqdsp.dataset.stations_data[:num_stations])\n",
    "        print(custom_labels)\n",
    "        # Set custom x-axis labels and rotate them\n",
    "        plt.xticks(ticks=5 * np.arange(0, len(custom_labels)), labels=custom_labels, rotation=90)\n",
    "\n",
    "        plt.xlabel(\"Station (latitude)\")\n",
    "        plt.ylabel(\"Sample Index\")\n",
    "        plt.title(f\"Drum plot with Clusters for first {num_stations} stations Z in the 127 station dataset\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_raw_data_with_cluster_colours_nonshuffled(self, proportion:float = 1):\n",
    "        end = int(self.tst_st.raw_data_with_clusters.shape[0] * proportion)\n",
    "        # Define a list of colors (expand this list if you have more clusters)\n",
    "        colors = ['red', 'blue', 'green', 'black', 'purple','orange', 'yellow', 'pink', 'brown', 'grey']\n",
    "        # Plot each segment colored by its cluster\n",
    "        for i in range(1, end):\n",
    "            plt.plot([i - 1, i],\n",
    "                     [self.tst_st.raw_data_with_clusters.iloc[i - 1,0],\n",
    "                      self.tst_st.raw_data_with_clusters.iloc[i,0]],\n",
    "                     color=colors[int(self.tst_st.raw_data_with_clusters.iloc[i-1,-1])])\n",
    "        plt.xlabel('time (100ths of a second)')\n",
    "        plt.ylabel('Seismic Amplitude')\n",
    "        # make the title fit into the graph\n",
    "        plt.title(\n",
    "            f'Samples Colored by Cluster Assignment \\n{self.tst_st.training_structure.dataset_processed.dataset.stations} {self.tst_st.training_structure.dataset_processed.dataset.axis}')\n",
    "        # create a legend which maps from cluster number to color\n",
    "        legend_elements = []\n",
    "        for c in range(self.tst_st.num_clusters):\n",
    "            legend_elements.append(Line2D([0], [0], color=colors[c], lw=4, label=f'Cluster {c}'))\n",
    "       \n",
    "        plt.legend(handles=legend_elements)\n",
    "        # put the legend outside the plot\n",
    "        plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        # resize the plot to fit the legend\n",
    "        plt.subplots_adjust(right=0.7)\n",
    "        plt.show()\n",
    "\n",
    "    # high pass filter\n",
    "    # has to be static so that it can be used in the map function\n",
    "    @staticmethod\n",
    "    def LPF(data: np.ndarray = None):\n",
    "        # low pass filter\n",
    "        b, a = butter(BUTTER_ORDER, SAMPLING_RATE//2 - 1, btype='low',\n",
    "                      fs=SAMPLING_RATE)\n",
    "        # The filtfilt function performs this bidirectional filtering by\n",
    "        # applying the filter coefficients both forwards and backwards.\n",
    "        # This process effectively eliminates the phase distortion\n",
    "        # introduced by the filter and provides a more accurate\n",
    "        # representation of the filtered signal in the time domain.\n",
    "        filtered_data = filtfilt(b, a, data)\n",
    "        return filtered_data\n",
    "\n",
    "    # calculate the spectrogram of each 500 sample frame of the raw test data\n",
    "    # and average them by cluster type\n",
    "    def plot_spectrogram_by_cluster(self, proportion:float = 1):\n",
    "        frame_spectrograms = []\n",
    "        end = int(self.tst_st.training_structure.dataset_processed.dataset.combined_data.shape[0] * proportion)\n",
    "        # remove any part of end that is not divisible by BLOCK_SIZE\n",
    "        if end % BLOCK_SIZE != 0:\n",
    "            end = end - (end % BLOCK_SIZE)\n",
    "        f = np.fft.rfftfreq(BLOCK_SIZE, 1 / SAMPLING_RATE)\n",
    "        # find the index of f which equals 1\n",
    "        index_of_1 = list(f).index(1)\n",
    "        print(f\"{index_of_1=}\")\n",
    "        chunk_index = 0\n",
    "        for i in range(0, end, BLOCK_SIZE):\n",
    "            frame = self.tst_st.training_structure.dataset_processed.filtered_data[i:i+BLOCK_SIZE]\n",
    "            # hanning\n",
    "            # filter the frame so it has nothing above the nyquist frequency\n",
    "            frame = frame * np.hanning(len(frame))\n",
    "            Fx = np.fft.rfft(frame)\n",
    "            Fx = np.abs(Fx)\n",
    "            frame_spectrograms.append(Fx)\n",
    "            chunk_index += 1\n",
    "\n",
    "        # average the spectrograms by cluster type\n",
    "        cluster_spectrograms = []\n",
    "        for cluster in range(self.tst_st.num_clusters):\n",
    "            cluster_frames = [frame_spectrograms[i] for i in range(len(frame_spectrograms)) if\n",
    "                              self.tst_st.all_clusters.iloc[i, -1] == cluster]\n",
    "            if len(cluster_frames) > 0:\n",
    "                cluster_spectrograms.append(np.average(cluster_frames, axis=0))\n",
    "            else:\n",
    "                print(\"No frames in cluster\", cluster)\n",
    "                # append a zero array if there are no frames in this cluster\n",
    "                cluster_spectrograms.append(np.zeros(len(f)))\n",
    "        # plot the spectrograms\n",
    "        fig, axs = plt.subplots(self.tst_st.num_clusters, 1, figsize=(10, 15))\n",
    "\n",
    "        for cluster in range(self.tst_st.num_clusters):\n",
    "            zero_cutoff = cluster_spectrograms[cluster]\n",
    "            #print(f\"{zero_cutoff=}\")\n",
    "            #print(f\"{f=}\")\n",
    "            axs[cluster].plot(f[index_of_1:], zero_cutoff[index_of_1:])\n",
    "            axs[cluster].set_title(f\"Cluster {cluster}\")\n",
    "            # make the x and y scales logarithmic\n",
    "            axs[cluster].set_xscale('log')\n",
    "            axs[cluster].set_xlabel('Frequency (Hz)')\n",
    "            axs[cluster].set_ylabel('Amplitude')\n",
    "            axs[cluster].grid(True, which='both')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # function to plot a normalised cumulative count of cluster occurences,\n",
    "    # one for each cluster, but in the same graph\n",
    "    def get_cumulative_cluster_activity_multi(self, proportion: float = 1):\n",
    "        # get the cluster activity\n",
    "        cluster_activity_df = pd.DataFrame(\n",
    "            self.tst_st.training_structure.features_df['test'].iloc[:,-1].values)\n",
    "        for c in range(self.tst_st.num_clusters):\n",
    "            cluster_activity_df[f'Cluster Activity {c}'] = \\\n",
    "                [int(cluster_activity_df.iloc[i, 0] == c)\n",
    "                 for i in range(len(cluster_activity_df))]\n",
    "            # now calculate the cumulative sum of the cluster activity\n",
    "            cluster_activity_df[f'Cum Cluster Activity {c}'] = \\\n",
    "                cluster_activity_df[f'Cluster Activity {c}'].cumsum()\n",
    "        # plot the cumulative cluster activity on a single graph\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        for c in range(self.tst_st.num_clusters):\n",
    "            ax.plot(cluster_activity_df[f'Cum Cluster Activity {c}'],\n",
    "                    label=f'Cluster {c}')\n",
    "        ax.set_xlabel('Sample')\n",
    "        ax.set_ylabel('Cumulative Cluster Activity')\n",
    "        ax.set_title('Cumulative Cluster Activity')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # method to take as input a list of stations\n",
    "    # and a range of times,\n",
    "    # and to then get an ordered list of what percentage of time\n",
    "    # certain clusters are active\n",
    "    def get_cluster_activity_by_time(self, station_list: list[str]):\n",
    "        stations = self.tst_st.training_structure.dataset_processed.dataset.stations\n",
    "\n",
    "        # find station indices for the station list\n",
    "        station_indices = []\n",
    "        for station in station_list:\n",
    "            station_indices.append(stations.index(station))\n",
    "\n",
    "        A = self.tst_st.training_structure.dataset_processed.dataset.time_scale  # eqdsp.dataset.time_scale\n",
    "        B = self.tst_st.training_structure.dataset_processed.dataset.combined_data\n",
    "\n",
    "\n",
    "        C = self.tst_st.all_clusters.iloc[:, -1].to_numpy()\n",
    "        print(f\"{C=}\")\n",
    "\n",
    "        # although seeking out cluster type, we will only\n",
    "        # focus on the 48 hours again.\n",
    "\n",
    "        # A = eqdsp.dataset.time_scale\n",
    "\n",
    "        # for a 48 hour set\n",
    "        # A[i][1] is location of 9am-ish\n",
    "        # A [i][3] is the end of the data\n",
    "        # So location of next 9am-ish is 1*A[i][3] + A[i+1][1]\n",
    "        # So location of next 9am-ish is 2*A[i+1][3] + A[i+2][1]\n",
    "        # or looking into past\n",
    "        # So location of next 9am-ish is i*A[i-1][3] + A[i][1]\n",
    "\n",
    "        old_A = A\n",
    "        new_A = [0 * 0 + old_A[0][1]]\n",
    "        for i in range(1, len(A)):\n",
    "            #print(old_A[i])\n",
    "            try:\n",
    "                new_A.append(i * old_A[i - 1][3] + old_A[i][1] + 1)\n",
    "            except:\n",
    "                # # the none, is to exclude this data\n",
    "                # the 30000*2 is what should've been excluded from 48 hours\n",
    "                old_A[i] = [0,0,0,17220000+30000*2,None]\n",
    "        A = new_A\n",
    "\n",
    "        # remove all station data that is not in the stations list\n",
    "        A_stations = []\n",
    "        for i in range(len(A)):\n",
    "            if i in station_indices:\n",
    "                if len(old_A[i]) == 4:\n",
    "                    A_stations.append(A[i])\n",
    "                else:\n",
    "                    print(f\"None valid station data at station index {station_indices.index(i)}\")\n",
    "        A = A_stations\n",
    "        # A contains the indices of the start of each 9am for each piece of data\n",
    "        # for the stations in the station list\n",
    "        print(f\"{A=}\")\n",
    "        # on the 48 hour clock, daytime is 9am to 5pm\n",
    "        # nighttime midnight to 5am\n",
    "        daytime = [9,17]\n",
    "        nighttime = [24,29]\n",
    "        day_samples = []\n",
    "        night_samples = []\n",
    "        convert_to_samples = SAMPLING_RATE*60*60\n",
    "        for i in range(len(A)):\n",
    "            t0 = daytime[0]\n",
    "            t1 = daytime[1]\n",
    "            sample_index_9am = A[i]+(t0 - 9)*convert_to_samples\n",
    "            sample_index_5pm = A[i] + (t1 - 9) * convert_to_samples\n",
    "            day_samples.append((sample_index_9am, sample_index_5pm))\n",
    "            t0 = nighttime[0]\n",
    "            t1 = nighttime[1]\n",
    "            sample_index_midnight = A[i] + (t0 - 9) * convert_to_samples\n",
    "            sample_index_5am = A[i] + (t1 - 9) * convert_to_samples\n",
    "            night_samples.append((sample_index_midnight, sample_index_5am))\n",
    "        #print(f\"{day_samples=}\")\n",
    "        #print(f\"{night_samples=}\")\n",
    "\n",
    "        cluster_count = {'day':dict(), 'night':dict()}\n",
    "        cluster_count['day'] = {'0':0, '1':0, '2':0, '3':0, '4':0}\n",
    "        cluster_count['night'] = {'0': 0, '1': 0, '2': 0, '3': 0, '4': 0}\n",
    "        sample_set = dict()\n",
    "        sample_set['day'] = day_samples\n",
    "        sample_set['night'] = night_samples\n",
    "        print(sample_set)\n",
    "        for t in ('day', 'night'):\n",
    "            #print(f\"{sample_set[t]=}\")\n",
    "            for s_e in sample_set[t]:\n",
    "                s,e  = s_e\n",
    "                chunk_s = math.floor(s/BLOCK_SIZE)\n",
    "                chunk_e = math.floor(e/BLOCK_SIZE)\n",
    "                clusters_s_e = C[chunk_s:chunk_e]\n",
    "                # count the number of times each cluster appears\n",
    "                for cluster in clusters_s_e:\n",
    "                    cluster_count[t][str(cluster)] += 1\n",
    "        print(f\"{cluster_count=}\")\n",
    "\n",
    "\n",
    "    def cumulative_cluster_plot_by_station(self, station: str):\n",
    "        stations = self.tst_st.training_structure.dataset_processed.dataset.stations\n",
    "\n",
    "        station_index = stations.index(station)\n",
    "\n",
    "        A = self.tst_st.training_structure.dataset_processed.dataset.time_scale  # eqdsp.dataset.time_scale\n",
    "        B = self.tst_st.training_structure.dataset_processed.dataset.combined_data\n",
    "\n",
    "        C = self.tst_st.all_clusters.iloc[:, -1].to_numpy()\n",
    "        print(f\"{C=}\")\n",
    "\n",
    "        # although seeking out cluster type, we will only\n",
    "        # focus on the 48 hours again.\n",
    "\n",
    "        # for a 48 hour set\n",
    "        # A[i][1] is location of 9am-ish\n",
    "        # A [i][3] is the end of the data\n",
    "        # So location of next 9am-ish is 1*A[i][3] + A[i+1][1]\n",
    "        # So location of next 9am-ish is 2*A[i+1][3] + A[i+2][1]\n",
    "        # or looking into past\n",
    "        # So location of next 9am-ish is i*A[i-1][3] + A[i][1]\n",
    "\n",
    "        old_A = A\n",
    "        num_nones = 0\n",
    "        new_A = [0 * 0 + old_A[0][1]]\n",
    "        for i in range(1, len(A)):\n",
    "            # print(old_A[i])\n",
    "            try:\n",
    "                new_A.append(i * old_A[i - 1][3] + old_A[i][1] + 1)\n",
    "            except:\n",
    "                # # the none, is to exclude this data\n",
    "                # the 30000*2 is what should've been excluded from 48 hours\n",
    "                old_A[i] = [0, 0, 0, 17220000 + 30000 * 2, None]\n",
    "        A = new_A\n",
    "\n",
    "        # remove all station data that is not in the stations list\n",
    "        A_station = A[station_index]\n",
    "        A = A_station\n",
    "        # A contains the indices of the start of each 9am for each piece of data\n",
    "        # for the stations in the station list\n",
    "        print(f\"{A=}\")\n",
    "        cluster_time_series = {'0': [0], '1': [0], '2': [0], '3': [0], '4': [0]}\n",
    "        convert_to_samples = SAMPLING_RATE * 60 * 60\n",
    "        start = math.floor(A / BLOCK_SIZE)\n",
    "        end = math.floor((A + convert_to_samples * TEST_DATA_HOURS)/BLOCK_SIZE)\n",
    "        for c_i in range(start, end):\n",
    "            cluster = C[c_i]\n",
    "            for i in cluster_time_series.keys():\n",
    "                if str(cluster) == i:\n",
    "                    cluster_time_series[i].append(\n",
    "                        cluster_time_series[i][-1]+1)\n",
    "                else:\n",
    "                    cluster_time_series[str(i)].append(\n",
    "                        cluster_time_series[str(i)][-1])\n",
    "\n",
    "        for i in cluster_time_series.keys():\n",
    "            x_data = np.round(np.linspace(0, 48, len(cluster_time_series[i])))\n",
    "            #x_data = np.linspace(0, 48, len(cluster_time_series[i]))\n",
    "            plt.plot(x_data, cluster_time_series[i], label=f\"Cluster {i}\")\n",
    "        plt.legend()\n",
    "        plt.ylabel(\"Cumulative Cluster Count\")\n",
    "        plt.xlabel(\"Time (Hours)\")\n",
    "        plt.title(\"Cumulative Cluster Count for 48 hours for Station \" + station)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ca570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ae4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOL FUNCTIONS FOR TRAINING\n",
    "\n",
    "def build_and_save_processed_data(\n",
    "        json_file: str = '',\n",
    "        h5_file: str = '', axis: str = 'Z',\n",
    "        start_hour: float = 0, num_hours: float = 0,\n",
    "        exclude_file: str = '', only_include: int = 999):\n",
    "    if json_file == \"\":\n",
    "        eqdssc = EQDataSetSingleSensorCombined(h5_file,\n",
    "                                                  axis,\n",
    "                                                  start_hour,\n",
    "                                                  num_hours,\n",
    "                                                  exclude_file=exclude_file,\n",
    "                                                  only_include=only_include)\n",
    "                                                    #'markups_all_IWEH_Z.csv')\n",
    "    else:\n",
    "        eqdsj = EQDataSetJSON(json_file)\n",
    "        eqdsj.get_data()\n",
    "        eqdssc = eqdsj.generate_EQDataSetCombined(axis=axis)\n",
    "\n",
    "    print(f\"{eqdssc=}\")\n",
    "    eqdsp = EQDataSetProcessor(eqdssc)\n",
    "    eqdsp.compute_features_matrix()\n",
    "    print(f\"{eqdsp=}\")\n",
    "    eqdsp.plot_features(0.0001)\n",
    "\n",
    "    # extract day month year from current datetime\n",
    "    # and use it to create a unique filename\n",
    "    # for the pickle file\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d%m%Y\")\n",
    "    if json_file == \"\":\n",
    "        op_filename = f\"EQDataSetProcessor({h5_file}, {axis}, {start_hour},\" +\\\n",
    "                      f\"{num_hours})_{dt_string})_only_include_{only_include}.pkl\"\n",
    "    else:\n",
    "        op_filename = f\"EQDataSetProcessor({json_file})_{dt_string}.pkl\"\n",
    "    print(eqdsp)\n",
    "    print(\"Pickling processor data...\")\n",
    "\n",
    "    with open(op_filename, 'wb') as f:\n",
    "        pickle.dump(eqdsp, f)\n",
    "    return eqdsp\n",
    "\n",
    "\n",
    "def get_station_data(station_name='IWEH', filename='all_stations.json' ):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for d in data:\n",
    "        if d['name'][:4] == station_name:\n",
    "            return d\n",
    "    return None\n",
    "\n",
    "\n",
    "def switch_to_disk():\n",
    "    path_to_external_drive = \"/Volumes/LaCie\"\n",
    "\n",
    "    # Check if the path exists (i.e., if the disk is connected and mounted)\n",
    "    if os.path.exists(path_to_external_drive):\n",
    "        os.chdir(path_to_external_drive)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"Path '{path_to_external_drive}' does not exist. Ensure the external disk is connected.\")\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "def get_all_ev_files():\n",
    "     all_files = os.listdir()\n",
    "     ev_files = []\n",
    "     for file in all_files:\n",
    "         if file[-3:] == '.h5' and file[:2] == 'ev':\n",
    "             ev_files.append(file)\n",
    "     return ev_files\n",
    "\n",
    "\n",
    "def convert_to_eq_json_format(d: dict, start=0, num_hours=0,\n",
    "                              axis='Z',json_filename=\"auto_generated_experiment.json\"):\n",
    "    json_list = []\n",
    "    for key in d:   # for each event file\n",
    "        for station in d[key]:\n",
    "            json_list.append({\n",
    "                \"event_filename\": key,\n",
    "                \"station\": station,\n",
    "                \"axis\": axis,\n",
    "                \"start_hour\": start,\n",
    "                \"num_hours\": num_hours,\n",
    "                \"exclude_file\": \"\"\n",
    "            })\n",
    "    # write json_list as a json file\n",
    "    json_data = json.dumps(json_list, indent=4)\n",
    "    #switch_to_internal()\n",
    "    with open(json_filename, 'w') as f:\n",
    "        f.write(json_data)\n",
    "        \n",
    "def build_station_dict(json_filename=\"auto_generated_experiment.json\",\n",
    "                       start=48, num_hours=48,axis='Z',\n",
    "                       events=[]):\n",
    "    # load in the ev hfiles one by one and extract all station names\n",
    "    # and store them in a list\n",
    "    stations = {}\n",
    "    stations_set = set()\n",
    "    station_total = 0\n",
    "    for filename in get_all_ev_files():\n",
    "        # empty events means do all events\n",
    "        if len(events) > 0 and filename not in events:\n",
    "            continue\n",
    "        print(f\"{filename=}\")\n",
    "        sub_stations = []\n",
    "        with h5py.File(filename, 'r') as file:\n",
    "            for s in file:\n",
    "                sub_stations.append(s)\n",
    "                stations_set.add(s)\n",
    "                station_total += 1\n",
    "                print(s, end = \",\")\n",
    "        stations[filename] = sub_stations\n",
    "        print()\n",
    "    print(stations)\n",
    "\n",
    "    convert_to_eq_json_format(stations, start=start,\n",
    "                              axis=axis, num_hours=num_hours,json_filename=json_filename)\n",
    "    print(f\"Total number of stations: {len(stations_set)} ({station_total})\")\n",
    "\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000d8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "SKIP_PROCESSING = False\n",
    "if not SKIP_PROCESSING:\n",
    "    # this one is the 75+52 files\n",
    "    file_to_load = \"EQDataSetProcessor(experiment_2.1.json)_21082023.pkl\"\n",
    "\n",
    "    load_processed_from_file = False\n",
    "    if load_processed_from_file:\n",
    "        print(\"\\nLoading EQDataSetProcessor object from pickle file...\\n\")\n",
    "        with open(file_to_load, 'rb') as f:\n",
    "            eqdsp = pickle.load(f)\n",
    "    else:\n",
    "\n",
    "        events = ['ev0001903830.h5', 'ev0000593283.h5']\n",
    "        axis='Z'\n",
    "        exp_filename = 'experiment_2.1.json'\n",
    "        #exp_filename = 'experiment_2.1_minitest.json'\n",
    "        switch_to_disk()\n",
    "\n",
    "        build_station_dict(json_filename=exp_filename, start=48, num_hours=48,\n",
    "                              axis=axis, events=events)\n",
    "        switch_to_disk()\n",
    "        eqdsp = build_and_save_processed_data(json_file=exp_filename,\n",
    "                                              axis=axis)\n",
    "    print(eqdsp)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aff374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_training_from_file = False\n",
    "if not load_training_from_file:\n",
    "    eqdsc = EQDataSetClusterTrain(eqdsp)\n",
    "    print(eqdsc)\n",
    "    dt_string = datetime.now().strftime(\"%d%m%Y\")\n",
    "    op_filename = f\"eqdsc.run_training(defaults)_on_EQDataSetProcessor(\"\n",
    "    op_filename += file_to_load\n",
    "    op_filename += f\"_at_{dt_string}.pkl\"\n",
    "\n",
    "    print(\"\\nRunning Training..\\n\")\n",
    "    eqdsc.run_training()\n",
    "\n",
    "    print(\"\\nSaving EQDataSetClusterTrain object to pickle file...\\n\")\n",
    "    with open(op_filename, 'wb') as f:\n",
    "        pickle.dump(eqdsc, f)\n",
    "    print(eqdsc)\n",
    "else:\n",
    "    print(\"\\nLoading EQDataSetClusterTrain object from pickle file...\\n\")\n",
    "    #training_file_to_load = \"eqdsc.run_training(defaults)_on_EQDataSetProcessor(EQDataSetProcessor(ev0000364000.h5, Z, 0, 0)_13082023.pkl_at_{dt_string}.pkl\" #\"eqdsc.run_training(defaults)_on_EQDataSetProcessor(ev0000364000.h5, Z, 0, 0)_09082023_at_09082023.pkl\"\n",
    "    training_file_to_load = \"eqdsc.run_training(defaults)_on_EQDataSetProcessor(EQDataSetProcessor(experiment_2.1.json)_21082023.pkl_at_23082023.pkl\"  # \"eqdsc.run_training(defaults)_on_EQDataSetProcessor(ev0000364000.h5, Z, 0, 0)_09082023_at_09082023.pkl\"\n",
    "    with open(training_file_to_load, 'rb') as f:\n",
    "        eqdsc = pickle.load(f)\n",
    "\n",
    "\n",
    "print(\"DONE\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd17339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_tests_from_file = False\n",
    "if not load_tests_from_file:\n",
    "    eqdsct = EQDataSetClusterTest(eqdsc, num_clusters=5)\n",
    "    eqdsct.cluster_test()\n",
    "    #with open(\"eqdsct.run_test(defaults)_on_EQDataSetClusterTrain(eqdsc.run_training(defaults)_on_EQDataSetProcessor(EQDataSetProcessor(experiment_2.1.json)_21082023.pkl_at_23082023.pkl)_at_27082023.pkl\", 'wb') as f:\n",
    "    #    pickle.dump(eqdsct, f)\n",
    "else:\n",
    "    with open(\"eqdsct.run_test(defaults)_on_EQDataSetClusterTrain(eqdsc.run_training(defaults)_on_EQDataSetProcessor(EQDataSetProcessor(experiment_2.1.json)_21082023.pkl_at_23082023.pkl)_at_27082023.pkl\", 'rb') as f:\n",
    "        eqdsct = pickle.load(f)\n",
    "\n",
    "\n",
    "# need to rebuild the processing structure\n",
    "eqdsct.training_structure.dataset_processed = eqdsp\n",
    "\n",
    "\n",
    "# mapping back to original data\n",
    "eqdsct.get_original_cluster_feature_rows()\n",
    "print(eqdsct.training_structure.features_df['test'])\n",
    "#eqdsct.get_original_cluster_raw_rows()\n",
    "\n",
    "# set up the plot object\n",
    "eqdsctp = EQDataSetClusterTestPlot(eqdsct)\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0862d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot time evolution of clusters cumulatively for a single station \n",
    "#eqdsctp.cumulative_cluster_plot_by_station('KMAH')\n",
    "\n",
    "print(\"Preparing plot with colours\")\n",
    "# display the first 0.0001 of all the data with cluster colours\n",
    "#eqdsctp.plot_raw_data_with_cluster_colours(0.0001)\n",
    "# display the first 10 stations\n",
    "eqdsctp.drum_plot_with_cluster_colours(0.0001)\n",
    "\n",
    "# average spectrogram\n",
    "print(\"Preparing plot of spectrogram\")\n",
    "eqdsctp.plot_spectrogram_by_cluster()\n",
    "#sys.exit()\n",
    "\n",
    "geo_coastal = [\"MSMH\", \"NCNH\", \"NOBH\", \"YMMH\", \"YMDH\", \"NMEH\", \"KMIH\", \"KKWH\", \"KAKH\"]\n",
    "\n",
    "geo_inland =[\"THTH\", \"SZKH\", \"MGMH\", \"KMYH\", \"NMNH\", \"OGNH\", \"NRKH\", \"SBAH\", \"SUKH\"]\n",
    "\n",
    "geo_major_urban = ['FKSH','KMAH','KORH','KTMH', 'NGSH', 'RIFH','SDWH','IMRH','YHBH']\n",
    "geo_non_urban = ['FSWH','GKSH','HKSH','HNRH','HRDH','ICEH','YMDH','YMGH','YUZH']\n",
    "\"\"\"\n",
    "print(\"Coastal stations\")\n",
    "eqdsctp.get_cluster_activity_by_time(geo_coastal)\n",
    "print(\"Inland stations\")\n",
    "eqdsctp.get_cluster_activity_by_time(geo_inland)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "print(\"Urban stations\")\n",
    "eqdsctp.get_cluster_activity_by_time(geo_major_urban)\n",
    "print(\"Non-urban stations\")\n",
    "eqdsctp.get_cluster_activity_by_time(geo_non_urban)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA \"MINING\" TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are tools that were used to select the data for analysis\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points\n",
    "    on the Earth's surface given their latitude and longitude in decimal degrees.\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    # Radius of Earth in kilometers. Use 3956 for miles\n",
    "    r = 6371.\n",
    "    # Calculate the distance\n",
    "    distance = r * c\n",
    "    return round(distance,0)\n",
    "\n",
    "\n",
    "def get_station_overlaps(stations):\n",
    "    # for each eventfile key, find the intersection of the stations with every other\n",
    "    # eventfile key using set conversion operations\n",
    "    exclude_list = ['ev0001903830.h5']\n",
    "    filtered_station_items = {k: v for k, v in stations.items() if k not in exclude_list}\n",
    "    #print(filtered_station_items)\n",
    "    for i in filtered_station_items.items():\n",
    "        print(f\"{i[0]} intersects with:\", end=\" \"   )\n",
    "        for j in filtered_station_items.items():\n",
    "            if i[0] != j[0]:\n",
    "                #print(f\"Intersection of {i[0]} and {j[0]}: {set(i[1]) & set(j[1])}\")\n",
    "                if len(set(i[1]) & set(j[1])) > 0:\n",
    "                    print(j[0], end=\", \")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "def calculate_station_distance():\n",
    "    # use the json file of station data to calculate the distance between each station\n",
    "    # and every other station\n",
    "    station_distances = dict()\n",
    "    with open(\"all_stations.json\", 'r') as f:\n",
    "        stations = json.load(f)\n",
    "    station_pairs_done = []\n",
    "\n",
    "    for station in stations:\n",
    "        for station2 in stations:\n",
    "            if station != station2:\n",
    "                # append this pair to the list of pairs done as a set\n",
    "                # so that we don't do the same pair twice\n",
    "                if {station['name'][:4], station2['name'][:4]} not in station_pairs_done:\n",
    "                    station_pairs_done.append({station['name'][:4], station2['name'][:4]})\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #print(station2)\n",
    "                station_distances[\n",
    "                    station['name']+\", \"+station2['name']] = (\n",
    "                    haversine_distance(station['latitude'], station['longitude'],\n",
    "                                         station2['latitude'], station2['longitude']))\n",
    "    #print(station_distances)\n",
    "    #sys.exit()\n",
    "    # generate list which is sorted by distance\n",
    "    sorted_distances = sorted(station_distances.items(), key=lambda x: -x[1])\n",
    "    return sorted_distances\n",
    "\n",
    "sd = calculate_station_distance()\n",
    "events = ['ev0000364000', 'ev0000447288']\n",
    "for i in sd:\n",
    "    if (events[0] in i[0] and events[1] in i[0]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPPING TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a base map\n",
    "m = folium.Map(location=[36.2454, 138.5472], zoom_start=5.5, tiles='CartoDB Positron')\n",
    "\n",
    "with open('experiment_2.1.json', 'r') as f:\n",
    "    experiment = json.load(f)\n",
    "with open('all_stations.json', 'r') as f:\n",
    "    all_stations = json.load(f)\n",
    "\n",
    "locations = []\n",
    "\n",
    "stations = []\n",
    "for station in experiment:\n",
    "    stations.append(station['station'][:4])\n",
    "\n",
    "for station in all_stations:\n",
    "    if station['name'][:4] in stations:\n",
    "        locations.append([station['latitude'], station['longitude']])\n",
    "\n",
    "print(locations)\n",
    "\n",
    "\n",
    "for location in locations:\n",
    "    folium.CircleMarker(\n",
    "        location=location,\n",
    "        radius=3,  # Adjust the size by setting the radius\n",
    "        color=\"blue\",\n",
    "        fill=True,\n",
    "        fill_color=\"blue\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('stations_mapped.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ce7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
